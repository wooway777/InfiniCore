#include "../../../devices/bang/bang_handle.h"
#include "../../../devices/bang/common_bang.h"
#include "../../../tensor.h"
#include "rearrange_bang.h"

namespace op::rearrange::bang {

struct Descriptor::Opaque {
    utils::RearrangeMeta meta;
    size_t element_size;
    int *d_idx_strides;
    int *d_dst_strides;
    int *d_src_strides;
};

Descriptor::~Descriptor() {
    if (_opaque) {
        cnrtFree(_opaque->d_idx_strides);
        cnrtFree(_opaque->d_dst_strides);
        cnrtFree(_opaque->d_src_strides);
        delete _opaque;
    }
}

__mlu_global__ void rearrange(
    char *dst,
    const char *src,
    const int *idx_strides,
    const int *dst_strides,
    const int *src_strides,
    int ndim,
    int count,
    int unit_size) {

    const int task_size = (count + taskDim - 1) / taskDim;
    const int start = taskId * task_size;
    const int end = (start + task_size) < count ? (start + task_size) : count;

    for (int i = start; i < end; ++i) {
        int rem = i;
        int dst_offset = 0;
        int src_offset = 0;

        for (int j = 0; j < ndim; ++j) {
            const int k = rem / idx_strides[j];
            dst_offset += k * dst_strides[j];
            src_offset += k * src_strides[j];
            rem %= idx_strides[j];
        }

        __memcpy(dst + dst_offset, src + src_offset, unit_size, GDRAM2GDRAM);
    }
}

infiniStatus_t Descriptor::create(
    infiniopHandle_t handle_,
    Descriptor **desc_ptr,
    infiniopTensorDescriptor_t y_desc,
    infiniopTensorDescriptor_t x_desc) {

    auto handle = reinterpret_cast<device::bang::Handle *>(handle_);
    auto dtype = y_desc->dtype();
    auto ndim = y_desc->ndim();

    // Validate input and output tensors
    auto y_shape = y_desc->shape();
    auto x_shape = x_desc->shape();
    CHECK_OR_RETURN(x_desc->dtype() == dtype, INFINI_STATUS_BAD_TENSOR_DTYPE);
    CHECK_OR_RETURN(x_desc->ndim() == ndim, INFINI_STATUS_BAD_TENSOR_SHAPE);
    CHECK_SAME_SHAPE(x_shape, y_shape);

    // Get strides and element size
    auto dst_strides = y_desc->strides();
    auto src_strides = x_desc->strides();
    auto element_size = infiniSizeOf(dtype);

    // Create rearrange meta
    auto meta_result = utils::RearrangeMeta::create(
        y_shape.data(),
        dst_strides.data(),
        src_strides.data(),
        ndim,
        element_size);

    CHECK_RESULT(meta_result);

    // Convert stride arrays to 32-bit
    std::vector<int> idx_strides(meta_result->ndim());
    std::vector<int> dst_strides_32(meta_result->ndim());
    std::vector<int> src_strides_32(meta_result->ndim());

    for (size_t i = 0; i < meta_result->ndim(); ++i) {
        idx_strides[i] = static_cast<int>(meta_result->idx_strides()[i]);
        dst_strides_32[i] = static_cast<int>(meta_result->dst_strides()[i]);
        src_strides_32[i] = static_cast<int>(meta_result->src_strides()[i]);
    }

    // Allocate device memory for strides
    int *d_idx_strides, *d_dst_strides, *d_src_strides;
    cnrtRet_t ret;
    ret = cnrtMalloc((void **)&d_idx_strides, idx_strides.size() * sizeof(int));
    CHECK_OR_RETURN(ret == cnrtSuccess, INFINI_STATUS_INTERNAL_ERROR);
    ret = cnrtMalloc((void **)&d_dst_strides, dst_strides_32.size() * sizeof(int));
    CHECK_OR_RETURN(ret == cnrtSuccess, INFINI_STATUS_INTERNAL_ERROR);
    ret = cnrtMalloc((void **)&d_src_strides, src_strides_32.size() * sizeof(int));
    CHECK_OR_RETURN(ret == cnrtSuccess, INFINI_STATUS_INTERNAL_ERROR);

    // Create queue for async operations
    cnrtQueue_t queue;
    cnrtQueueCreate(&queue);

    // Copy stride data to device
    cnrtMemcpyAsync(d_idx_strides, idx_strides.data(),
                    idx_strides.size() * sizeof(int), queue, cnrtMemcpyHostToDev);
    cnrtMemcpyAsync(d_dst_strides, dst_strides_32.data(),
                    dst_strides_32.size() * sizeof(int), queue, cnrtMemcpyHostToDev);
    cnrtMemcpyAsync(d_src_strides, src_strides_32.data(),
                    src_strides_32.size() * sizeof(int), queue, cnrtMemcpyHostToDev);
    cnrtQueueSync(queue);
    cnrtQueueDestroy(queue);

    // Create opaque data
    auto opaque = new Opaque{
        meta_result.take(),
        element_size,
        d_idx_strides,
        d_dst_strides,
        d_src_strides};

    *desc_ptr = new Descriptor(
        meta_result.take(),
        opaque,
        handle->device,
        handle->device_id);

    return INFINI_STATUS_SUCCESS;
}

infiniStatus_t Descriptor::calculate(
    void *y,
    const void *x,
    void *stream) const {

    cnrtQueue_t queue = reinterpret_cast<cnrtQueue_t>(stream);
    auto &meta = _opaque->meta;

    // Configure kernel launch parameters
    cnrtDim3_t dim;
    cnrtFunctionType_t func_type;

    // Use union mode for better parallelism
    dim.x = 4; // Using 4 clusters
    dim.y = 1;
    dim.z = 1;
    func_type = CNRT_FUNC_TYPE_UNION1;

    // Launch kernel
    rearrange<<<dim, func_type, queue>>>(
        reinterpret_cast<char *>(y),
        reinterpret_cast<const char *>(x),
        _opaque->d_idx_strides,
        _opaque->d_dst_strides,
        _opaque->d_src_strides,
        static_cast<int>(meta.ndim()),
        static_cast<int>(meta.count()),
        static_cast<int>(meta.unit()));

    cnrtQueueSync(queue);
    return INFINI_STATUS_SUCCESS;
}

} // namespace op::rearrange::bang